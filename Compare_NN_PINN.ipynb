{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c149d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "import functools\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import torch.autograd as autograd\n",
    "\n",
    "\n",
    "from typing import Callable\n",
    "from torch import nn, optim\n",
    "from data import diffeq, create_trainig_test_set, create_trainig_validation_test_set\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "646bf6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the harmonic oscillator\n",
    "\n",
    "def f(t,x,k):\n",
    "    #write the function that is on the other side of the differential equation\n",
    "    return -k*x\n",
    "\n",
    "def differential_equation_2_order(t, X, k):\n",
    "    #rewrite d^2x/dt^2 as dy/dt where y=dx/dt\n",
    "    x, y = X\n",
    "    dx_dt = y\n",
    "    dy_dt = f(t,x,k)\n",
    "\n",
    "    return [dx_dt, dy_dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d90943c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.accelerator.is_available():\n",
    "    device = torch.accelerator.current_accelerator().type\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81eccb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define NN\n",
    "\n",
    "class HarmonicModel(nn.Module):\n",
    "    def __init__(self, n_in, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        act = {\n",
    "            'tanh': nn.Tanh(),\n",
    "            'sine': torch.sin,\n",
    "            'gelu': nn.GELU(),\n",
    "            'relu': nn.ReLU()\n",
    "        }\n",
    "\n",
    "        #allow for different sizes of neurons for each layer\n",
    "        layer_sizes = [n_in] + hidden_sizes + [1] #becomes for example [4, 64, 32, 16, 1]\n",
    "        \n",
    "        self.net = nn.ModuleList([\n",
    "            nn.Linear(layer_sizes[i], layer_sizes[i+1])\n",
    "            for i in range(len(layer_sizes)-1)\n",
    "        ]) #nn.Linear(4, 64), nn.Linear(64, 32), ..., nn.Linear(16, 1)\n",
    "\n",
    "        self.activation = act[activation] #chooses a value from the dictionary, e.g. act['tanh']\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.net[:-1]:\n",
    "            if callable(self.activation):  # sine special case\n",
    "                x = self.activation(layer(x))\n",
    "            else:\n",
    "                x = self.activation(layer(x))\n",
    "        return self.net[-1](x)\n",
    "    \n",
    "def loss_harmonic(y_pred, y_exp):\n",
    "    mse_y = torch.mean((y_pred - y_exp)**2)\n",
    "    \n",
    "    return mse_y\n",
    "\n",
    "#evaluate model\n",
    "def evaluate_NN(model, data):\n",
    "    model.eval()\n",
    "    X_val = data[\"X\"].to(device)\n",
    "    y_val = data[\"y_tensor\"][:, :, 0].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_val).squeeze(-1)\n",
    "        loss = loss_harmonic(y_pred, y_val)\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bd672d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define PINN\n",
    "\n",
    "class HarmonicModel2(nn.Module):\n",
    "    def __init__(self, n_in, n_layers, n_neurons, activation):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        act = {\n",
    "            'tanh': nn.Tanh(),\n",
    "            'sine': torch.sin,\n",
    "            'gelu': nn.GELU(),\n",
    "            'relu': nn.ReLU()\n",
    "        }[activation]\n",
    "        \n",
    "        layers.append(nn.Linear(n_in, n_neurons))\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(nn.Linear(n_neurons, n_neurons))\n",
    "        layers.append(nn.Linear(n_neurons, 1))\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.activation = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            if callable(self.activation):  # sine special case\n",
    "                x = self.activation(layer(x))\n",
    "            else:\n",
    "                x = self.activation(layer(x))\n",
    "        return self.layers[-1](x)\n",
    "\n",
    "def loss_PINN2(model, y_pred, y_exp, x_00, x_01, t, k, N_f):\n",
    "    mse_y = torch.mean((y_pred - y_exp)**2)\n",
    "\n",
    "    t_f = torch.linspace(0, 100, steps=N_f).repeat(len(t), 1).unsqueeze(-1).requires_grad_(True).to(device)\n",
    "\n",
    "    n_rep = int(N_f / len(y_exp[0]))\n",
    "\n",
    "    k_f = k.unsqueeze(-1).repeat(1, n_rep, 1)\n",
    "    x_00_f = x_00.unsqueeze(-1).repeat(1, n_rep, 1)\n",
    "    x_01_f = x_01.unsqueeze(-1).repeat(1, n_rep, 1)\n",
    "\n",
    "    X_f = torch.cat([k_f, x_00_f, x_01_f, t_f], dim=-1)\n",
    "    \n",
    "    y_f = model(X_f)\n",
    "    x_t = autograd.grad(y_f, t_f,\n",
    "                       grad_outputs=torch.ones_like(y_f),\n",
    "                       create_graph=True, allow_unused=True)[0]\n",
    "    \n",
    "    x_tt = autograd.grad(x_t, t_f,\n",
    "                        grad_outputs=torch.ones_like(x_t),\n",
    "                        create_graph=True, allow_unused=True)[0]\n",
    "    \n",
    "    # Residual\n",
    "    f_res = x_tt + k_f*y_f\n",
    "    mse_f = torch.mean(f_res**2)\n",
    "\n",
    "    return mse_y + mse_f\n",
    "\n",
    "#evaluate model\n",
    "def evaluate_PINN(model, data, N_f=1000):\n",
    "    model.eval()\n",
    "\n",
    "    X = data[\"X\"].to(device)\n",
    "    y_exp = data[\"y_tensor\"][0:, 0:, 0].to(device)\n",
    "\n",
    "    y_pred = model(X).squeeze(-1)\n",
    "    t = X[0:, 0:, 3]\n",
    "    k = X[0:, 0:, 0]\n",
    "    x_00 = X[0:, 0:, 1]\n",
    "    x_01 = X[0:, 0:, 2]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X).squeeze(-1)\n",
    "        loss = loss_PINN2(model, y_pred, y_exp, x_00, x_01, t, k, N_f)\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596cde38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load models\n",
    "\n",
    "# Model class must be defined somewhere\n",
    "model_NN = torch.load(\"optimized_model_NN.pt\", weights_only=False)\n",
    "model_NN.eval()  # put in evaluation mode\n",
    "model_PINN = torch.load(\"optimized_model_PINN.pt\", weights_only=False, map_location=torch.device('cpu'))\n",
    "model_PINN.eval()  # put in evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4ae3aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define dataset to compare models\n",
    "\n",
    "torch.manual_seed(0) #get the same random numbers everytime you run the code\n",
    "harmonic_osc = diffeq(differential_equation_2_order, 2, 1)\n",
    "\n",
    "t_span = (0, 10) #wanted timespan\n",
    "n_step = 100 #number of steps to train the data (number of initial conditions)\n",
    "n_data = 1000 #number of data per step (per initial condition, this number of datapoints to train, validate, test data)\n",
    "\n",
    "#validation set will be used to choose hyperparameters, decide when to stop training, compare multiple models fairly and detect overfitting\n",
    "train, test = create_trainig_test_set(harmonic_osc,\n",
    "                                      t_span=t_span,\n",
    "                                      n_steps=n_step,\n",
    "                                      n_data=n_data,\n",
    "                                      coeff_test=0.3,\n",
    "                                      method=\"RK45\",\n",
    "                                      device=device,\n",
    "                                      seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08e01539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test loss on the optimized model is 0.07714784890413284\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m loss_opt_NN = evaluate_NN(model_NN, data=test)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe test loss on the optimized model is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_opt_NN\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m loss_opt_PINN = \u001b[43mevaluate_PINN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_PINN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe test loss on the optimized model is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_opt_PINN\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mevaluate_PINN\u001b[39m\u001b[34m(model, data, N_f)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_PINN\u001b[39m(model, data, N_f=\u001b[32m1000\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m()\n\u001b[32m     62\u001b[39m     X = data[\u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m].to(device)\n\u001b[32m     63\u001b[39m     y_exp = data[\u001b[33m\"\u001b[39m\u001b[33my_tensor\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m:, \u001b[32m0\u001b[39m:, \u001b[32m0\u001b[39m].to(device)\n",
      "\u001b[31mAttributeError\u001b[39m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "#evaluate models\n",
    "\n",
    "#evaluate\n",
    "loss_opt_NN = evaluate_NN(model_NN, data=test)\n",
    "print(f\"The test loss on the optimized model is {loss_opt_NN}\")\n",
    "\n",
    "loss_opt_PINN = evaluate_PINN(model_PINN, data=test)\n",
    "print(f\"The test loss on the optimized model is {loss_opt_PINN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55cdd92",
   "metadata": {},
   "source": [
    "# Performance on noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f64f2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare the performance on noisy data\n",
    "\n",
    "def add_noise_to_dataset(dataset, tensor_keys=['y_tensor'], noise_std=0.05, seed=None):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to specified tensors in a dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (dict): Dictionary of tensors, e.g., {'y_tensor': ..., 't_tensor': ...}\n",
    "        tensor_keys (list): Which tensors to add noise to (usually target/output tensors)\n",
    "        noise_std (float): Standard deviation of Gaussian noise\n",
    "        seed (int, optional): Random seed for reproducibility\n",
    "    Returns:\n",
    "        noisy_dataset (dict): Copy of dataset with added noise\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    noisy_dataset = {k: v.clone() for k, v in dataset.items()}  # copy original tensors\n",
    "    \n",
    "    for key in tensor_keys:\n",
    "        noisy_dataset[key] += torch.randn_like(dataset[key]) * noise_std\n",
    "    \n",
    "    return noisy_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21835428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noisy data (create only 1 set to check the model performances)\n",
    "train_no_noise, test_no_noise = create_trainig_test_set(harmonic_osc,\n",
    "                                                        t_span=(0, 10),\n",
    "                                                        n_steps=100,\n",
    "                                                        n_data=1,\n",
    "                                                        coeff_valtest=[0.699, 0.001, 0.3],\n",
    "                                                        method=\"RK45\",\n",
    "                                                        device=device,\n",
    "                                                        seed=0)\n",
    "\n",
    "# Example: add noise with std=0.1 to y_tensor\n",
    "noise_std = 0.5\n",
    "train_noise = add_noise_to_dataset(train_no_noise, tensor_keys=['y_tensor'], noise_std=noise_std, seed=0)\n",
    "test_noise = add_noise_to_dataset(test_no_noise, tensor_keys=['y_tensor'], noise_std=noise_std, seed=0)\n",
    "\n",
    "# test dataset bayes\n",
    "X = test_noise[\"X\"]\n",
    "\n",
    "#get prediction for NN and PINN\n",
    "y_pred_noise_NN = model_NN(X).squeeze(-1)\n",
    "y_pred_noise_PINN = model_PINN(X).squeeze(-1)\n",
    "\n",
    "test_loss_noise_NN = evaluate_NN(model_NN, data=test_noise)\n",
    "print(f\"Test Loss Random optimization: {test_loss_noise_NN:.4f}\")\n",
    "test_loss_noise_PINN = evaluate_NN(model_PINN, data=test_noise)\n",
    "print(f\"Test Loss Random optimization: {test_loss_noise_PINN:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5ef852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the results\n",
    "n = 0\n",
    "\n",
    "y_pred_noise_NN.to(\"cpu\")\n",
    "y_pred_noise_PINN.to(\"cpu\")\n",
    "test_noise[\"t_tensor\"].to(\"cpu\")\n",
    "\n",
    "#Solve numerically\n",
    "y0 = [test_noise[\"x0_tensor\"][n][0], test_noise[\"x0_tensor\"][n][1]]  # initial condition [x0, v0]\n",
    "t_span = test_noise[\"t_tensor\"][n][0], test_noise[\"t_tensor\"][n][-1]\n",
    "w = test_noise[\"args_tensor\"][n]\n",
    "\n",
    "diff = differential_equation_2_order\n",
    "sol = sp.integrate.solve_ivp(diff, t_span, y0, t_eval=test_noise[\"t_tensor\"][n], method='RK45', args=(w))\n",
    "\n",
    "# Create figure with GridSpec\n",
    "fig = plt.figure(figsize=(7, 9))\n",
    "gs = fig.add_gridspec(2, 1, height_ratios=[3, 1], hspace=0)\n",
    "\n",
    "#create ratios for lower plot\n",
    "slice = 5 #so that the plot is clear\n",
    "ratio_an_num = (test_no_noise[\"y_tensor\"][n][0:, 0]/sol.y[0])[::slice]\n",
    "ratio_an_NN = (test_no_noise[\"y_tensor\"][n][0:, 0]/y_pred_noise_NN.cpu().detach()[n])[::slice]\n",
    "ratio_an_PINN = (test_no_noise[\"y_tensor\"][n][0:, 0]/y_pred_noise_PINN.cpu().detach()[n])[::slice]\n",
    "\n",
    "# Main plot\n",
    "ax_main = fig.add_subplot(gs[0])\n",
    "ax_main.scatter(test_noise[\"t_tensor\"][n], test_noise[\"y_tensor\"][n][0:, 0], label=\"Analytical solution\") #analytical\n",
    "ax_main.plot(sol.t, sol.y[0], label='Numerical solution', color = 'green') #numerical\n",
    "ax_main.plot(test_noise[\"t_tensor\"][n], y_pred_noise_NN.cpu().detach()[n], color=\"blue\", label=\"Predictions NN\") #NN ML\n",
    "ax_main.plot(test_noise[\"t_tensor\"][n], y_pred_noise_PINN.cpu().detach()[n], color=\"red\", label=\"Predictions PINN\") #PINN ML\n",
    "ax_main.set_ylabel('Predicted values')\n",
    "ax_main.legend()\n",
    "ax_main.grid(True)\n",
    "ax_main.tick_params(bottom=False)  # remove x-axis ticks for main plot\n",
    "\n",
    "# Ratio / comparison plot\n",
    "ax_ratio = fig.add_subplot(gs[1], sharex=ax_main)\n",
    "ax_ratio.plot(test_noise[\"t_tensor\"][n][::slice], ratio_an_num, '.', label='Analytical/Numerical', color='green')\n",
    "ax_ratio.plot(test_noise[\"t_tensor\"][n][::slice], ratio_an_NN, '.', label='Analytical/NN', color='blue')\n",
    "ax_ratio.plot(test_noise[\"t_tensor\"][n][::slice], ratio_an_PINN, '.', label='Analytical/PINN', color='red')\n",
    "ax_ratio.axhline(1, color='gray', linestyle='--')\n",
    "ax_ratio.set_xlabel('time (s)')\n",
    "ax_ratio.set_ylabel('Ratio')\n",
    "ax_ratio.set_ylim(0, 2)\n",
    "ax_ratio.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2312eb",
   "metadata": {},
   "source": [
    "# Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f55724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generalization\n",
    "train_generalization, test_generalization = create_trainig_test_set(harmonic_osc,\n",
    "                                                                    t_span=(0, 50),\n",
    "                                                                    n_steps=200,\n",
    "                                                                    n_data=1,\n",
    "                                                                    coeff_test=0.3,\n",
    "                                                                    method=\"RK45\",\n",
    "                                                                    device=device,\n",
    "                                                                    seed=0)\n",
    "# test dataset bayes\n",
    "X = test_generalization[\"X\"]\n",
    "y_pred_generalization_NN = model_NN(X).squeeze(-1)\n",
    "y_pred_generalization_PINN = model_PINN(X).squeeze(-1)\n",
    "\n",
    "\n",
    "test_loss_generalization_NN = evaluate_NN(model_NN, data=test_generalization)\n",
    "print(f\"Test Loss Random optimization: {test_loss_generalization_NN:.4f}\")\n",
    "test_loss_generalization_PINN = evaluate_NN(model_PINN, data=test_generalization)\n",
    "print(f\"Test Loss Random optimization: {test_loss_generalization_PINN:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea0823",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "\n",
    "y_pred_generalization_NN.to(\"cpu\")\n",
    "y_pred_generalization_PINN.to(\"cpu\")\n",
    "test_generalization[\"t_tensor\"].to(\"cpu\")\n",
    "\n",
    "#Solve numerically\n",
    "y0 = [test_generalization[\"x0_tensor\"][n][0], test_generalization[\"x0_tensor\"][n][1]]  # initial condition [x0, v0]\n",
    "t_span = test_generalization[\"t_tensor\"][n][0], test_generalization[\"t_tensor\"][n][-1]\n",
    "w = test_generalization[\"args_tensor\"][n]\n",
    "\n",
    "diff = differential_equation_2_order\n",
    "sol = sp.integrate.solve_ivp(diff, t_span, y0, t_eval=test_generalization[\"t_tensor\"][n], method='RK45', args=(w))\n",
    "\n",
    "# Create figure with GridSpec\n",
    "fig = plt.figure(figsize=(7, 9))\n",
    "gs = fig.add_gridspec(2, 1, height_ratios=[3, 1], hspace=0)\n",
    "\n",
    "#create ratios for lower plot\n",
    "slice = 5\n",
    "ratio_an_num = (test_generalization[\"y_tensor\"][n][0:, 0]/sol.y[0])[::slice]\n",
    "ratio_an_NN = (test_generalization[\"y_tensor\"][n][0:, 0]/y_pred_generalization_NN.cpu().detach()[n])[::slice]\n",
    "ratio_an_PINN = (test_generalization[\"y_tensor\"][n][0:, 0]/y_pred_generalization_PINN.cpu().detach()[n])[::slice]\n",
    "\n",
    "# Main plot\n",
    "ax_main = fig.add_subplot(gs[0])\n",
    "ax_main.scatter(test_generalization[\"t_tensor\"][n], test_generalization[\"y_tensor\"][n][0:, 0], label=\"Analytical solution\") #analytical\n",
    "ax_main.plot(sol.t, sol.y[0], label='Numerical solution', color = 'green') #numerical\n",
    "ax_main.plot(test_generalization[\"t_tensor\"][n], y_pred_generalization_NN.cpu().detach()[n], color=\"blue\", label=\"Predictions NN\") #NN ML\n",
    "ax_main.plot(test_generalization[\"t_tensor\"][n], y_pred_generalization_PINN.cpu().detach()[n], color=\"red\", label=\"Predictions PINN\") #PINN ML\n",
    "ax_main.set_ylabel('Distance')\n",
    "ax_main.legend()\n",
    "ax_main.grid(True)\n",
    "ax_main.tick_params(bottom=False)  # remove x-axis ticks for main plot\n",
    "\n",
    "# Ratio / comparison plot\n",
    "ax_ratio = fig.add_subplot(gs[1], sharex=ax_main)\n",
    "ax_ratio.plot(test_generalization[\"t_tensor\"][n][::slice], ratio_an_num, '.', label='Analytical/Numerical', color='green')\n",
    "ax_ratio.plot(test_generalization[\"t_tensor\"][n][::slice], ratio_an_NN, '.', label='Analytical/NN', color='blue')\n",
    "ax_ratio.plot(test_generalization[\"t_tensor\"][n][::slice], ratio_an_PINN, '.', label='Analytical/PINN', color='red')\n",
    "ax_ratio.axhline(1, color='gray', linestyle='--')\n",
    "ax_ratio.set_xlabel('t')\n",
    "ax_ratio.set_ylabel('Ratio')\n",
    "ax_ratio.set_ylim(0, 2)\n",
    "ax_ratio.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4e824d",
   "metadata": {},
   "source": [
    "# Energy conservation\n",
    "\n",
    "Add energy conservation of numerical method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ddbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0  # choose trajectory index\n",
    "\n",
    "# Position\n",
    "x_NN = y_pred_generalization_NN[n].detach().cpu().numpy()\n",
    "x_PINN = y_pred_generalization_PINN[n].detach().cpu().numpy()\n",
    "\n",
    "# Time\n",
    "t = test_generalization[\"t_tensor\"][n].detach().cpu().numpy()\n",
    "\n",
    "# Velocity via finite difference\n",
    "v_NN = np.gradient(x_NN, t)\n",
    "v_PINN = np.gradient(x_PINN, t)\n",
    "\n",
    "#Energy conservation (check on generalized data)\n",
    "E_t_NN = 0.5 * v_NN**2 + 0.5 * float(w) * x_NN**2\n",
    "E0_NN = E_t_NN[0]\n",
    "E_t_PINN = 0.5 * v_PINN**2 + 0.5 * float(w) * x_PINN**2\n",
    "E0_PINN = E_t_PINN[0]\n",
    "\n",
    "# diagnostics\n",
    "abs_dev_NN = np.max(np.abs(E_t_NN - E0_NN))\n",
    "rel_dev_NN = np.max(np.abs((E_t_NN - E0_NN) / E0_NN))\n",
    "abs_dev_PINN = np.max(np.abs(E_t_PINN - E0_PINN))\n",
    "rel_dev_PINN = np.max(np.abs((E_t_PINN - E0_PINN) / E0_PINN))\n",
    "\n",
    "print(f\"Initial energy NN E0 = {E0_NN:.6e}\")\n",
    "print(f\"Max absolute NN deviation = {abs_dev_NN:.6e}\")\n",
    "print(f\"Max relative NN deviation = {rel_dev_NN:.6e}\")\n",
    "\n",
    "print(f\"Initial energy NN E0 = {E0_PINN:.6e}\")\n",
    "print(f\"Max absolute NN deviation = {abs_dev_PINN:.6e}\")\n",
    "print(f\"Max relative NN deviation = {rel_dev_PINN:.6e}\")\n",
    "\n",
    "plt.plot(test_generalization[\"t_tensor\"][n], E_t_NN, label='E(t)', color=\"blue\")\n",
    "plt.plot(test_generalization[\"t_tensor\"][n], E_t_PINN, label='E(t)', color=\"red\")\n",
    "plt.hlines(E0_NN, sol.t[0], sol.t[-1], colors='k', linestyles='dashed', label='E0')\n",
    "#plt.hlines(E0_PINN, sol.t[0], sol.t[-1], colors='k', linestyles='dashed', label='E0') #both same EO?\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('Energy')\n",
    "plt.title('Energy vs time (check conservation)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38098c07",
   "metadata": {},
   "source": [
    "# Computational time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd24c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "\n",
    "#Computation time\n",
    "N = 200 #number of times we compute the time\n",
    "timesteps_NN = np.zeros(N)\n",
    "timesteps_PINN = np.zeros(N)\n",
    "timesteps_Num = np.zeros(N)\n",
    "N_list = np.arange(0,N)\n",
    "\n",
    "X = test[\"X\"][n]\n",
    "y0 = [test[\"x0_tensor\"][n][0], test[\"x0_tensor\"][n][1]]  # initial condition [x0, v0]\n",
    "t_span = test[\"t_tensor\"][n][0], test[\"t_tensor\"][n][-1]\n",
    "w = test[\"args_tensor\"][n]\n",
    "\n",
    "#NN\n",
    "for i, el in tqdm(enumerate(N_list), desc=\"Computation time NN: \"):\n",
    "    begin = time.perf_counter()\n",
    "    for _ in range(el):\n",
    "        model_NN(X)\n",
    "    end = time.perf_counter()\n",
    "    timesteps_NN[i] = end-begin\n",
    "\n",
    "#PINN\n",
    "for i, el in tqdm(enumerate(N_list), desc=\"Computation time PINN: \"):\n",
    "    begin = time.perf_counter()\n",
    "    for _ in range(el):\n",
    "        model_PINN(X)\n",
    "    end = time.perf_counter()\n",
    "    timesteps_PINN[i] = end-begin\n",
    "\n",
    "#Numerical\n",
    "for i, el in tqdm(enumerate(N_list), desc=\"Computation time Numerical: \"):\n",
    "    begin = time.perf_counter()\n",
    "    for _ in range(el):\n",
    "        sp.integrate.solve_ivp(diff, t_span, y0, t_eval=test[\"t_tensor\"][n], method='RK45', args=(w))\n",
    "    end = time.perf_counter()\n",
    "    timesteps_Num[i] = end-begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213704bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot results computational time\n",
    "plt.plot(N_list, timesteps_NN, color='blue', label=\"Neural Network\")\n",
    "plt.plot(N_list, timesteps_PINN, color='red', label=\"Physics Informed Neural Network\")\n",
    "plt.plot(N_list, timesteps_Num, color='green', label=\"Numerical Method\")\n",
    "plt.xlabel(\"Number of iterations N\")\n",
    "plt.ylabel(\"time t (s)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1f3e76",
   "metadata": {},
   "source": [
    "# Test performance in function of number of Training data\n",
    "\n",
    "Do this individually for NN and PINN. If needed we can just export the data separately and plot it then instead of saving the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML_WSL_env)",
   "language": "python",
   "name": "ml_wsl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
