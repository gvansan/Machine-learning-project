{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c149d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "import functools\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import torch.autograd as autograd\n",
    "\n",
    "\n",
    "from typing import Callable\n",
    "from torch import nn, optim\n",
    "from data import diffeq, create_trainig_test_set, create_trainig_validation_test_set\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "646bf6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the harmonic oscillator\n",
    "\n",
    "def f(t,x,k):\n",
    "    #write the function that is on the other side of the differential equation\n",
    "    return -k*x\n",
    "\n",
    "def differential_equation_2_order(t, X, k):\n",
    "    #rewrite d^2x/dt^2 as dy/dt where y=dx/dt\n",
    "    x, y = X\n",
    "    dx_dt = y\n",
    "    dy_dt = f(t,x,k)\n",
    "\n",
    "    return [dx_dt, dy_dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d90943c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.accelerator.is_available():\n",
    "    device = torch.accelerator.current_accelerator().type\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81eccb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define NN\n",
    "\n",
    "class HarmonicModel(nn.Module):\n",
    "    def __init__(self, n_in, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        act = {\n",
    "            'tanh': nn.Tanh(),\n",
    "            'sine': torch.sin,\n",
    "            'gelu': nn.GELU(),\n",
    "            'relu': nn.ReLU()\n",
    "        }\n",
    "\n",
    "        #allow for different sizes of neurons for each layer\n",
    "        layer_sizes = [n_in] + hidden_sizes + [1] #becomes for example [4, 64, 32, 16, 1]\n",
    "        \n",
    "        self.net = nn.ModuleList([\n",
    "            nn.Linear(layer_sizes[i], layer_sizes[i+1])\n",
    "            for i in range(len(layer_sizes)-1)\n",
    "        ]) #nn.Linear(4, 64), nn.Linear(64, 32), ..., nn.Linear(16, 1)\n",
    "\n",
    "        self.activation = act[activation] #chooses a value from the dictionary, e.g. act['tanh']\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.net[:-1]:\n",
    "            if callable(self.activation):  # sine special case\n",
    "                x = self.activation(layer(x))\n",
    "            else:\n",
    "                x = self.activation(layer(x))\n",
    "        return self.net[-1](x)\n",
    "    \n",
    "def loss_harmonic(y_pred, y_exp):\n",
    "    mse_y = torch.mean((y_pred - y_exp)**2)\n",
    "    \n",
    "    return mse_y\n",
    "\n",
    "#evaluate model\n",
    "def evaluate_NN(model, data):\n",
    "    model.eval()\n",
    "    X_val = data[\"X\"].to(device)\n",
    "    y_val = data[\"y_tensor\"][:, :, 0].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_val).squeeze(-1)\n",
    "        loss = loss_harmonic(y_pred, y_val)\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bd672d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define PINN\n",
    "\n",
    "class HarmonicModel2(nn.Module):\n",
    "    def __init__(self, n_in, n_layers, n_neurons, activation):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        act = {\n",
    "            'tanh': nn.Tanh(),\n",
    "            'sine': torch.sin,\n",
    "            'gelu': nn.GELU(),\n",
    "            'relu': nn.ReLU()\n",
    "        }[activation]\n",
    "        \n",
    "        layers.append(nn.Linear(n_in, n_neurons))\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(nn.Linear(n_neurons, n_neurons))\n",
    "        layers.append(nn.Linear(n_neurons, 1))\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.activation = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            if callable(self.activation):  # sine special case\n",
    "                x = self.activation(layer(x))\n",
    "            else:\n",
    "                x = self.activation(layer(x))\n",
    "        return self.layers[-1](x)\n",
    "\n",
    "def loss_PINN2(model, y_pred, y_exp, x_00, x_01, t, k, N_f):\n",
    "    mse_y = torch.mean((y_pred - y_exp)**2)\n",
    "\n",
    "    t_f = torch.linspace(0, 100, steps=N_f).repeat(len(t), 1).unsqueeze(-1).requires_grad_(True).to(device)\n",
    "\n",
    "    n_rep = int(N_f / len(y_exp[0]))\n",
    "\n",
    "    k_f = k.unsqueeze(-1).repeat(1, n_rep, 1)\n",
    "    x_00_f = x_00.unsqueeze(-1).repeat(1, n_rep, 1)\n",
    "    x_01_f = x_01.unsqueeze(-1).repeat(1, n_rep, 1)\n",
    "\n",
    "    X_f = torch.cat([k_f, x_00_f, x_01_f, t_f], dim=-1)\n",
    "    \n",
    "    y_f = model(X_f)\n",
    "    x_t = autograd.grad(y_f, t_f,\n",
    "                       grad_outputs=torch.ones_like(y_f),\n",
    "                       create_graph=True, allow_unused=True)[0]\n",
    "    \n",
    "    x_tt = autograd.grad(x_t, t_f,\n",
    "                        grad_outputs=torch.ones_like(x_t),\n",
    "                        create_graph=True, allow_unused=True)[0]\n",
    "    \n",
    "    # Residual\n",
    "    f_res = x_tt + k_f*y_f\n",
    "    mse_f = torch.mean(f_res**2)\n",
    "\n",
    "    return mse_y + mse_f\n",
    "\n",
    "#evaluate model\n",
    "def evaluate_PINN(model, data, N_f=1000):\n",
    "    model.eval()\n",
    "\n",
    "    X = data[\"X\"].to(device)\n",
    "    y_exp = data[\"y_tensor\"][0:, 0:, 0].to(device)\n",
    "\n",
    "    y_pred = model(X).squeeze(-1)\n",
    "    t = X[0:, 0:, 3]\n",
    "    k = X[0:, 0:, 0]\n",
    "    x_00 = X[0:, 0:, 1]\n",
    "    x_01 = X[0:, 0:, 2]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X).squeeze(-1)\n",
    "        loss = loss_PINN2(model, y_pred, y_exp, x_00, x_01, t, k, N_f)\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596cde38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load models\n",
    "\n",
    "# Model class must be defined somewhere\n",
    "model_NN = torch.load(\"optimized_model_NN.pt\", weights_only=False)\n",
    "model_NN.eval()  # put in evaluation mode\n",
    "model_PINN = torch.load(\"optimized_model_PINN.pt\", weights_only=False, map_location=torch.device('cpu'))\n",
    "model_PINN.eval()  # put in evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4ae3aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define dataset to compare models\n",
    "\n",
    "torch.manual_seed(0) #get the same random numbers everytime you run the code\n",
    "harmonic_osc = diffeq(differential_equation_2_order, 2, 1)\n",
    "\n",
    "t_span = (0, 10) #wanted timespan\n",
    "n_step = 100 #number of steps to train the data (number of initial conditions)\n",
    "n_data = 1000 #number of data per step (per initial condition, this number of datapoints to train, validate, test data)\n",
    "\n",
    "#validation set will be used to choose hyperparameters, decide when to stop training, compare multiple models fairly and detect overfitting\n",
    "train, test = create_trainig_test_set(harmonic_osc,\n",
    "                                      t_span=t_span,\n",
    "                                      n_steps=n_step,\n",
    "                                      n_data=n_data,\n",
    "                                      coeff_test=0.3,\n",
    "                                      method=\"RK45\",\n",
    "                                      device=device,\n",
    "                                      seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08e01539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test loss on the optimized model is 0.07714784890413284\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m loss_opt_NN = evaluate_NN(model_NN, data=test)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe test loss on the optimized model is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_opt_NN\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m loss_opt_PINN = \u001b[43mevaluate_PINN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_PINN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe test loss on the optimized model is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_opt_PINN\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mevaluate_PINN\u001b[39m\u001b[34m(model, data, N_f)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_PINN\u001b[39m(model, data, N_f=\u001b[32m1000\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m()\n\u001b[32m     62\u001b[39m     X = data[\u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m].to(device)\n\u001b[32m     63\u001b[39m     y_exp = data[\u001b[33m\"\u001b[39m\u001b[33my_tensor\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m:, \u001b[32m0\u001b[39m:, \u001b[32m0\u001b[39m].to(device)\n",
      "\u001b[31mAttributeError\u001b[39m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "#evaluate models\n",
    "\n",
    "#evaluate\n",
    "loss_opt_NN = evaluate_NN(model_NN, data=test)\n",
    "print(f\"The test loss on the optimized model is {loss_opt_NN}\")\n",
    "\n",
    "loss_opt_PINN = evaluate_PINN(model_PINN, data=test)\n",
    "print(f\"The test loss on the optimized model is {loss_opt_PINN}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML_WSL_env)",
   "language": "python",
   "name": "ml_wsl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
